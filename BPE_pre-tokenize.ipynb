{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import regex as re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from typing import Tuple, List, Iterable, BinaryIO\n",
    "from collections import Counter\n",
    "from bpe_tokenizer import BytePairEncodingTokenizer\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e11b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1001: One day, a big dog named Max saw a small cat named Lily on top of a tree. Lily was angry because she could not get down. Max wanted to help Lily, so he thought of a plan.\n",
      "Line 1002: Max said, \"Lily, I will join you on top of the tree and help you get down.\" Max climbed up the tree and slowly got closer to Lily. Lily was scared at first, but Max was kind and gentle.\n",
      "Line 1003: Max said, \"Hold on to me, Lily. I will take you down.\" Lily held on tight to Max, and they went down the tree together. Lily was happy and thanked Max for helping her. From that day on, Max and Lily became the best of friends.\n",
      "Line 1004: <|endoftext|>\n",
      "Line 1005: Once upon a time, there was a white shark. The white shark lived in the big sea. One day, the white shark saw a little boat. The little boat had a hole in it. The white shark wanted to help.\n",
      "Line 1006: The white shark swam to the boat. The white shark said, \"I can fix your boat.\" The man in the boat was scared. The man said, \"No, go away!\" The white shark was sad but still wanted to help.\n",
      "Line 1007: The white shark tried to fix the hole with its nose. But the boat broke more. The man fell into the water. The white shark was very sorry, but it was too late. The boat was gone and the man was sad.\n",
      "Line 1008: <|endoftext|>\n",
      "Line 1009: Once upon a time, there was a dependable dog named Spot. Spot loved to play with his best friend, a little girl named Lucy. One day, they were playing in the yard when they found a big pear tree.\n",
      "Line 1010: Lucy said, \"Look, Spot! A pear tree! Let's pick a pear!\" They picked the biggest pear they could reach. The pear was very big and very yummy-looking. Spot and Lucy wanted to share the pear with their friends, so they went to find them.\n",
      "Line 1011: They found their friends, a cat named Whiskers and a bird named Tweetie, playing nearby. Lucy said, \"We found a big pear! Let's all share it!\" Whiskers and Tweetie were happy to share the pear. They all took turns taking bites of the pear, and they all agreed that it was the best pear they had ever tasted.\n",
      "Line 1012: As they were eating, a squirrel came up to them and said, \"That pear belongs to me! I picked it and dropped it by accident.\" Lucy, Spot, Whiskers, and Tweetie felt bad for taking the squirrel's pear. They all said sorry to the squirrel.\n",
      "Line 1013: The squirrel said, \"It's okay, we can all share the pear.\" So, they all sat together and finished eating the big, yummy pear. They all became friends and played together every day under the pear tree.\n",
      "Line 1014: <|endoftext|>\n",
      "Line 1015: \n",
      "Line 1016: It was a cold winter day. The snow covered the ground and the tall trees.\n",
      "Line 1017: Jason, who was three years old, wanted to stay outside and play in the snow. He wrapped his warm coat closer to his body and built a snowman with his dad.\n",
      "Line 1018: \"Look daddy!\" he shouted, pointing at the snowman.\n",
      "Line 1019: Dad smiled. \"It looks great.\"\n",
      "Line 1020: Just then, Jason saw a small smoke coming out of a chimney of the nearest house. He said, \"Let's hope someone will give us hot chocolate.\"\n",
      "Line 1021: Dad smiled again. \"That would be nice wouldn't it?\"\n",
      "Line 1022: Jason smiled and nodded. \"Yes, please. I'm so weak from playing in the snow.\"\n",
      "Line 1023: But when they reached the house, nobody answered the door. Jason was disappointed.\n",
      "Line 1024: \"Let's hope someone else will help us,\" said Dad.\n",
      "Line 1025: Just then, an old man came out of a nearby house. He said, \"Why, hello there. Let's warm up with some hot chocolate!\"\n",
      "Line 1026: Jason smiled. He was very happy. Now he was hopeful that he could still get his hot chocolate!\n",
      "Line 1027: <|endoftext|>\n",
      "Line 1028: \n",
      "Line 1029: Once upon a time, there was a poor man called Joe. He had a mustache and was very nice. One day, Joe went to the park to meet a friend.\n",
      "Line 1030: When Joe got to the park, he saw a little girl. She was sitting on the swings, crying.\n",
      "Line 1031: Joe went over to the little girl and asked her why she was crying. The girl told Joe that she was sad because she didn't know her own name.\n",
      "Line 1032: Joe was very kind and gentle and said to the little girl, \"Don't worry. I will help you find your name.\"\n",
      "Line 1033: Joe and the little girl searched all around the park, but they couldn't find her name. Joe was getting tired and the little girl was starting to cry again.\n",
      "Line 1034: Then, suddenly, Joe remembered something. He put his hand on the little girl's head and said, \"I know your name! Your name is Julie!\"\n",
      "Line 1035: The little girl smiled and said, \"Thank you Joe for finding my name!\" Joe smiled back, patted her on the head and said, \"You're welcome\".\n",
      "Line 1036: <|endoftext|>\n",
      "Line 1037: \n",
      "Line 1038: \n",
      "Line 1039: Ben and Lily liked to design things with their blocks. They made houses, cars, towers and animals. They had fun playing with their designs and making noises.\n",
      "Line 1040: One day, they wanted to design a big castle. They used all their blocks and worked hard. They made walls, doors, windows and a flag. They were proud of their castle and wanted to show it to their mom.\n",
      "Line 1041: But when they went to look for their mom, they saw a young boy in their room. He was their cousin, Tom. He had come to visit with his mom. He was bored and saw the blocks. He did not know that Ben and Lily had designed a castle. He thought they were just blocks. He knocked down the castle and threw the blocks around. He laughed and said, \"This is fun!\"\n",
      "Line 1042: Ben and Lily came back and saw what Tom had done. They were very sad and angry. They shouted, \"Tom, you are bad! You broke our castle! We worked hard to design it! Go away!\"\n",
      "Line 1043: Tom did not care. He said, \"I don't care. They are just blocks. You can make another castle. I want to play with them. Give them to me!\"\n",
      "Line 1044: He tried to grab the blocks from Ben and Lily. They did not want to share. They pushed him away. They started to fight. They hit, kicked and bit each other. They cried and screamed.\n",
      "Line 1045: Their mom heard the noise and came to see what was wrong. She saw the mess and the fight. She was very upset and disappointed. She said, \"Stop it, all of you! You are being naughty and rude! You are not playing nicely! You are making me unhappy!\"\n",
      "Line 1046: She took away the blocks and put them in a box. She said, \"You cannot play with the blocks anymore. You have to go to your rooms and think about what you did. You have to say sorry to each other and to me. You have to learn to share and respect each other's designs.\"\n",
      "Line 1047: She sent them to their rooms. They felt uncomfortable and ashamed. They did not want to say sorry. They did not want to share. They did not want to design anything anymore. They had lost their fun and their castle. They had a bad day.\n",
      "Line 1048: <|endoftext|>\n",
      "Line 1049: \n",
      "Line 1050: Once upon a time, there was a strong lion. He had a problem. He wanted to build a big tall house in his jungle, but he was stuck. He didn't know how to solve it.\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/scratch/shayan/Projects/LLMfromScratch/data/TinyStoriesV2-GPT4-train.txt\"\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 1000:\n",
    "            continue\n",
    "        if i >= 1050:\n",
    "            break\n",
    "        print(f\"Line {i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75a304a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2226845268"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e18a297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-tokenize the data regex-based GPT-2 style \n",
    "from tqdm import tqdm\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "TOKEN_BYTES = b\"<|endoftext|>\"\n",
    "\n",
    "# chunk_size = 1000000\n",
    "# tokens = []\n",
    "\n",
    "# for i in tqdm(range(0, len(data), chunk_size), desc=\"pre-tokenizing the vocabulary\"):\n",
    "#     chunk = data[i:i+chunk_size]\n",
    "#     tokens.extend(re.findall(PAT, chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bpe(vocab, merges, output_dir, data_name):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if isinstance(vocab, dict):\n",
    "        vocab_json = vocab\n",
    "    else:\n",
    "        raise TypeError(\"Vocabulary must be a dict of token->id\")\n",
    "    \n",
    "    with (output_dir/f\"{data_name}_vocab.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vocab_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    merges_path = output_dir / f\"{data_name}_merges.txt\"\n",
    "    with merges_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for a, b in merges:\n",
    "            f.write(f\"{a} {b}\\n\")\n",
    "\n",
    "def train_bpe_tinystories(data_path, vocab_size=10000, special_tokens=[\"<|endoftext|>\"], out_dir=\"tokenizer\"):\n",
    "    bpe = BytePairEncodingTokenizer(data_path)\n",
    "    vocabulary, merges = bpe.train_bpe(data_path, vocab_size=vocab_size, special_tokens=[\"<|endoftext|>\"])\n",
    "    save_bpe(vocabulary, merges, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b022e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length: 257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing chunks: 100%|██████████| 24/24 [00:26<00:00,  1.08s/it]\n",
      "Training BPE...: 100%|██████████| 9743/9743 [34:15<00:00,  4.74it/s, last merge: 10 chars]\n"
     ]
    }
   ],
   "source": [
    "train_bpe_tinystories(data_path, vocab_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f34979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1531, 431, 259, 2569, 387, 420, 2330, 1003, 2020, 377, 1553, 370, 259, 374, 81, 70, 266, 1343, 940, 282, 47, 309, 283, 378, 2929, 266, 613, 432, 263, 7278, 844, 116]\n"
     ]
    }
   ],
   "source": [
    "with open(\"tokenizer/vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "with open(\"tokenizer/merges.txt\", \"r\") as f:\n",
    "    merges = f.read()\n",
    "\n",
    "tokenizer = BytePairEncodingTokenizer.from_files(vocab_path=\"tokenizer/vocab.json\", merges_path=\"tokenizer/merges.txt\")\n",
    "\n",
    "text = \"This is a test for an interesting implementation of a BPE tokenizer. it was very exciting to learn all the detials\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cda26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BytePairEncodingTokenizer.from_files(vocab_path=\"tokenizer/vocab.json\", merges_path=\"tokenizer/merges.txt\")\n",
    "\n",
    "roundtrip = tokenizer.decode(tokenizer.encode(\"hello world!\"))\n",
    "assert roundtrip == \"hello world!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed5b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "571dc2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class SGDOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, **args):\n",
    "        if \"lr\" not in args:\n",
    "            raise KeyError(\"learning rate not provided\")\n",
    "        elif args[\"lr\"] < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {args['lr']}\")\n",
    "        \n",
    "        super().__init__(params, args)\n",
    "        \n",
    "    \n",
    "    def step(self, closure = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"] # get learning rate\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[p]\n",
    "                t = state.get(\"t\", 0)\n",
    "                grad = p.grad.data\n",
    "                p.data -= lr / math.sqrt(t+1) * grad # update weight tensor in-place\n",
    "                state[\"t\"] = t + 1\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d94a4b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.98601722717285\n",
      "26.87777328491211\n",
      "26.122926712036133\n",
      "25.52312660217285\n",
      "25.015216827392578\n",
      "24.56973648071289\n",
      "24.170150756835938\n",
      "23.806114196777344\n",
      "23.47063446044922\n",
      "23.158737182617188\n",
      "22.866724014282227\n",
      "22.591772079467773\n",
      "22.3316593170166\n",
      "22.0846004486084\n",
      "21.849132537841797\n",
      "21.624059677124023\n",
      "21.408361434936523\n",
      "21.201171875\n",
      "21.001758575439453\n",
      "20.80947494506836\n",
      "20.623764038085938\n",
      "20.44413948059082\n",
      "20.270160675048828\n",
      "20.101449966430664\n",
      "19.937658309936523\n",
      "19.77847671508789\n",
      "19.623624801635742\n",
      "19.472850799560547\n",
      "19.325929641723633\n",
      "19.182647705078125\n",
      "19.042814254760742\n",
      "18.90625\n",
      "18.77280044555664\n",
      "18.642311096191406\n",
      "18.514644622802734\n",
      "18.38967514038086\n",
      "18.26728057861328\n",
      "18.147354125976562\n",
      "18.029788970947266\n",
      "17.914491653442383\n",
      "17.801368713378906\n",
      "17.690338134765625\n",
      "17.58131980895996\n",
      "17.47423553466797\n",
      "17.369022369384766\n",
      "17.265607833862305\n",
      "17.16393280029297\n",
      "17.063934326171875\n",
      "16.965557098388672\n",
      "16.868749618530273\n",
      "16.773460388183594\n",
      "16.679641723632812\n",
      "16.587247848510742\n",
      "16.496234893798828\n",
      "16.40656280517578\n",
      "16.318193435668945\n",
      "16.2310848236084\n",
      "16.145204544067383\n",
      "16.060518264770508\n",
      "15.976988792419434\n",
      "15.89459228515625\n",
      "15.813292503356934\n",
      "15.733061790466309\n",
      "15.653874397277832\n",
      "15.575704574584961\n",
      "15.498522758483887\n",
      "15.422306060791016\n",
      "15.34703254699707\n",
      "15.272679328918457\n",
      "15.199224472045898\n",
      "15.126644134521484\n",
      "15.054922103881836\n",
      "14.984034538269043\n",
      "14.91396713256836\n",
      "14.84469985961914\n",
      "14.776213645935059\n",
      "14.70849323272705\n",
      "14.641523361206055\n",
      "14.575284004211426\n",
      "14.50976276397705\n",
      "14.4449462890625\n",
      "14.380819320678711\n",
      "14.317364692687988\n",
      "14.254573822021484\n",
      "14.192427635192871\n",
      "14.13092041015625\n",
      "14.07003402709961\n",
      "14.009760856628418\n",
      "13.95008659362793\n",
      "13.891000747680664\n",
      "13.832493782043457\n",
      "13.774552345275879\n",
      "13.717167854309082\n",
      "13.660330772399902\n",
      "13.604031562805176\n",
      "13.548257827758789\n",
      "13.493005752563477\n",
      "13.438258171081543\n",
      "13.384016036987305\n",
      "13.3302640914917\n"
     ]
    }
   ],
   "source": [
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "opt = SGDOptimizer([weights], lr=1)\n",
    "for t in range(100):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5dee73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.545366287231445\n",
      "21.54528045654297\n",
      "21.54521942138672\n",
      "21.545167922973633\n",
      "21.545127868652344\n",
      "21.545089721679688\n",
      "21.545053482055664\n",
      "21.545021057128906\n",
      "21.54499053955078\n",
      "21.544960021972656\n",
      "21.544933319091797\n",
      "21.544906616210938\n",
      "21.544883728027344\n",
      "21.544858932495117\n",
      "21.544836044311523\n",
      "21.544815063476562\n",
      "21.544790267944336\n",
      "21.54477310180664\n",
      "21.544750213623047\n",
      "21.54473304748535\n",
      "21.54471206665039\n",
      "21.544694900512695\n",
      "21.544677734375\n",
      "21.544654846191406\n",
      "21.544639587402344\n",
      "21.544618606567383\n",
      "21.544605255126953\n",
      "21.544588088989258\n",
      "21.544572830200195\n",
      "21.544557571411133\n",
      "21.544540405273438\n",
      "21.544527053833008\n",
      "21.544513702392578\n",
      "21.544496536254883\n",
      "21.544483184814453\n",
      "21.544466018676758\n",
      "21.544450759887695\n",
      "21.5444393157959\n",
      "21.544424057006836\n",
      "21.544410705566406\n",
      "21.544397354125977\n",
      "21.54438018798828\n",
      "21.54436683654785\n",
      "21.544355392456055\n",
      "21.544340133666992\n",
      "21.544330596923828\n",
      "21.5443172454834\n",
      "21.54430389404297\n",
      "21.544292449951172\n",
      "21.544279098510742\n",
      "21.544265747070312\n",
      "21.54425621032715\n",
      "21.544240951538086\n",
      "21.544233322143555\n",
      "21.544218063354492\n",
      "21.544208526611328\n",
      "21.544198989868164\n",
      "21.544187545776367\n",
      "21.544178009033203\n",
      "21.544164657592773\n",
      "21.544153213500977\n",
      "21.544139862060547\n",
      "21.544130325317383\n",
      "21.54412078857422\n",
      "21.544111251831055\n",
      "21.544099807739258\n",
      "21.544090270996094\n",
      "21.544076919555664\n",
      "21.5440673828125\n",
      "21.544057846069336\n",
      "21.544048309326172\n",
      "21.544034957885742\n",
      "21.544029235839844\n",
      "21.544015884399414\n",
      "21.54400634765625\n",
      "21.543994903564453\n",
      "21.543983459472656\n",
      "21.543973922729492\n",
      "21.543964385986328\n",
      "21.543956756591797\n",
      "21.5439453125\n",
      "21.543935775756836\n",
      "21.543928146362305\n",
      "21.543916702270508\n",
      "21.543909072875977\n",
      "21.543899536132812\n",
      "21.54388999938965\n",
      "21.54387855529785\n",
      "21.543869018554688\n",
      "21.543861389160156\n",
      "21.543851852416992\n",
      "21.543846130371094\n",
      "21.543834686279297\n",
      "21.5438289642334\n",
      "21.5438175201416\n",
      "21.543811798095703\n",
      "21.543800354003906\n",
      "21.543794631958008\n",
      "21.543785095214844\n",
      "21.543777465820312\n"
     ]
    }
   ],
   "source": [
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "opt = SGDOptimizer([weights], lr=1e-4)\n",
    "for t in range(100):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bef08fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.563339233398438\n",
      "7423.36572265625\n",
      "1282132.0\n",
      "142623504.0\n",
      "11552503808.0\n",
      "729095274496.0\n",
      "37429369110528.0\n",
      "1610371546742784.0\n",
      "5.935485459785318e+16\n",
      "1.9059502178747023e+18\n",
      "5.403537580174921e+19\n",
      "1.3672669511718252e+21\n",
      "3.1154994746824176e+22\n",
      "6.44136524047905e+23\n",
      "1.2161926838581661e+25\n",
      "2.1087211020420337e+26\n",
      "3.373953763267254e+27\n",
      "5.002893941418034e+28\n",
      "6.901057082783179e+29\n",
      "8.885801829076122e+30\n",
      "1.0712480280033089e+32\n",
      "1.2125353432841755e+33\n",
      "1.2918096190649032e+34\n",
      "1.2983626051452588e+35\n",
      "1.2336651253205981e+36\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "opt = SGDOptimizer([weights], lr=1e3)\n",
    "for t in range(100):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21072c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22., 10.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68c3bfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([33., 33., 32., 30., 32.])\n",
    "torch.var(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cccdb28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000e+00, 1.6550e+04])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[33., 33., 32., 30., 32.], [30., 30., 320., 40., 30.]])\n",
    "\n",
    "torch.var(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fc235060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, beta_1=0.9, beta_2=0.999, weight_decay=1e-2):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        \n",
    "        defaults = {\"lr\": lr, \"beta_1\": beta_1, \"beta_2\": beta_2, \"weight_decay\": weight_decay}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure = None):\n",
    "        loss = None if closure is None else closure\n",
    "        eps = 1e-8\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr'] # get learning rate for param groups\n",
    "            lr_initial = lr\n",
    "            b1 = group[\"beta_1\"]\n",
    "            b2 = group[\"beta_2\"]\n",
    "            wd = group[\"weight_decay\"]\n",
    "            \n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state[\"m\"] = torch.zeros_like(p.grad.data) # first moment\n",
    "                    state[\"v\"] = torch.zeros_like(p.grad.data) # second moment\n",
    "                \n",
    "                t = state.get(\"t\", 0)\n",
    "                grad = p.grad.data\n",
    " \n",
    "                state[\"m\"] = b1 * state[\"m\"] + (1 - b1) * grad\n",
    "                state[\"v\"] = b2 * state[\"v\"] + (1 - b2) * (grad ** 2)\n",
    "                lr_t = lr * math.sqrt(1 - b2**(t+1)) / (1 - b1**(t+1))\n",
    "                \n",
    "                p.data -= lr_t * state[\"m\"] / (torch.sqrt(state[\"v\"]) + eps)\n",
    "                p.data -= lr * wd * p\n",
    "                state[\"t\"] = t + 1\n",
    "                \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2bf36ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class AdamW(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, beta_1=0.9, beta_2=0.999, eps=1e-8, weight_decay=1e-2):\n",
    "        defaults = dict(lr=lr, beta_1=beta_1, beta_2=beta_2, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            b1, b2 = group[\"beta_1\"], group[\"beta_2\"]\n",
    "            eps, wd = group[\"eps\"], group[\"weight_decay\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # === Initialization ===\n",
    "                if len(state) == 0:\n",
    "                    state[\"t\"] = 0\n",
    "                    state[\"m\"] = torch.zeros_like(p)\n",
    "                    state[\"v\"] = torch.zeros_like(p)\n",
    "\n",
    "                m, v = state[\"m\"], state[\"v\"]\n",
    "                state[\"t\"] += 1\n",
    "                t = state[\"t\"]\n",
    "\n",
    "                # === Moment updates ===\n",
    "                m.mul_(b1).add_(grad, alpha=1 - b1)\n",
    "                v.mul_(b2).addcmul_(grad, grad, value=1 - b2)\n",
    "\n",
    "                # === Compute bias-corrected learning rate (α_t) ===\n",
    "                lr_t = lr * math.sqrt(1 - b2**t) / (1 - b1**t)\n",
    "\n",
    "                # === Parameter update ===\n",
    "                p.data.addcdiv_(m, v.sqrt().add_(eps), value=-lr_t)\n",
    "\n",
    "                # === Decoupled weight decay (final step) ===\n",
    "                if wd != 0:\n",
    "                    p.data.add_(p.data, alpha=-lr * wd)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e2071f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t is 0 and loss is : 27.16983413696289\n",
      "t is 50 and loss is : 24.990659713745117\n",
      "t is 100 and loss is : 22.991731643676758\n",
      "t is 150 and loss is : 21.160966873168945\n",
      "t is 200 and loss is : 19.482091903686523\n",
      "t is 250 and loss is : 17.940616607666016\n",
      "t is 300 and loss is : 16.5238094329834\n",
      "t is 350 and loss is : 15.220439910888672\n",
      "t is 400 and loss is : 14.0205659866333\n",
      "t is 450 and loss is : 12.915322303771973\n",
      "t is 500 and loss is : 11.8967924118042\n",
      "t is 550 and loss is : 10.95785903930664\n",
      "t is 600 and loss is : 10.092111587524414\n",
      "t is 650 and loss is : 9.29373550415039\n",
      "t is 700 and loss is : 8.5574312210083\n",
      "t is 750 and loss is : 7.878375053405762\n",
      "t is 800 and loss is : 7.252130031585693\n",
      "t is 850 and loss is : 6.674632549285889\n",
      "t is 900 and loss is : 6.142139434814453\n",
      "t is 950 and loss is : 5.651205062866211\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "opt = AdamW([weights], lr=5e-3)\n",
    "for t in range(1000):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    if t % 50 == 0:\n",
    "        print(f\"t is {t} and loss is : {loss.cpu().item()}\")\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    \n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e0cba50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30., 30., 30., 30., 30.],\n",
       "        [30., 30., 30., 30., 30.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[10., 20., 30., 40., 50.], [10., 20., 30., 40., 50.]])\n",
    "a = torch.tensor([[1., 2., 3., 4., 5.], [10., 20., 30., 40., 50.]])\n",
    "torch.mean(x, dim=-1, keepdim=True).expand(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bdaf93ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10., 10., 10.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765673a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
