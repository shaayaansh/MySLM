{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e349d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from typing import Tuple, List, Iterable, BinaryIO\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e11b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1001: From then on, Tim always observed his surroundings and found many more treasures. He learned that being alert can lead to finding special things.\n",
      "Line 1002: <|endoftext|>\n",
      "Line 1003: Once upon a time, there was a little boat. The boat liked to go to the shore. One day, the boat saw a big load. The load was heavy and uncomfortable.\n",
      "Line 1004: The boat wanted to help. So, the boat took the load to the shore. The load made the boat very uncomfortable. The boat felt slow and tired.\n",
      "Line 1005: In the end, the boat could not carry the load anymore. The boat stopped moving and stayed on the shore. The boat was sad and uncomfortable forever.\n",
      "Line 1006: <|endoftext|>\n",
      "Line 1007: Once upon a time, there was a brave cow named Bessie. Bessie loved to skip and play in the big green field. She had many friends who liked to play with her. They would skip, run, and jump all day long.\n",
      "Line 1008: One day, Bessie saw a big truck come to the farm. The truck was taking the cows to a new place. Bessie was excited to go on a trip. She thought it would be fun to see new things and make new friends. So, she got in the truck with the other cows.\n",
      "Line 1009: When the truck stopped, Bessie found out that the new place was not fun at all. The cows were put in a small pen and they could not skip or play. Bessie missed her big green field and her friends. She was very sad and wished she could go back home. The unexpected thing was that the new place was not a happy place for cows like Bessie. The brave cow's adventure did not have a happy ending.\n",
      "Line 1010: <|endoftext|>\n",
      "Line 1011: \n",
      "Line 1012: Mommy was cooking dinner. She had a dish in her hands and was pouring blue liquid into it. Suddenly, she smiled and gave the dish to their three-year-old child who was standing nearby. The child was excited and hugged the big blue dish. Mommy had made a delicious blueberry pudding for the child and the child could not wait to eat it.\n",
      "Line 1013: The child then grabbed a spoon and started scooping some pudding into its mouth. Mommy smiled as she watched the child enjoy the dish. Together, they ate the blueberry pudding until there was none left in the dish.\n",
      "Line 1014: Mommy then took the empty dish and said she would make some more the next day. The child smiled, content to eat the delicious blueberry pudding.\n",
      "Line 1015: <|endoftext|>\n",
      "Line 1016: Once upon a time, a kind girl named Lily went for a walk. She saw a pretty purse on the ground. Lily liked the purse and wanted to find who it belonged to.\n",
      "Line 1017: Lily asked her friends, \"Do you know who lost this purse?\" Her friends did not know, but they all admired the purse. They thought it was very nice.\n",
      "Line 1018: Finally, Lily found a sad lady who had lost her purse. The lady was so happy when Lily gave it back to her. The lady said, \"Thank you, kind girl!\" Lily smiled and felt good for being helpful.\n",
      "Line 1019: <|endoftext|>\n",
      "Line 1020: One day, a yellow bird sat on a post. The bird saw a big cat. The cat saw the bird and wanted to catch it. The bird was scared and did not know what to do.\n",
      "Line 1021: The bird shouted, \"Help! Help! Big cat wants to catch me!\" A little boy heard the bird shout. He ran to the post and saw the yellow bird and the big cat. The little boy said, \"Go away, big cat! Leave the yellow bird alone!\"\n",
      "Line 1022: But the big cat did not listen. It jumped up and caught the yellow bird. The little boy was sad. The yellow bird was gone. The big cat walked away with the bird. The boy could not help the yellow bird.\n",
      "Line 1023: <|endoftext|>\n",
      "Line 1024: Once upon a time, there was a little boy named Tim. Tim was grumpy when he woke up in his bedroom. The sun was shining through the window. The sun was saying, \"Rise, Tim! Time to play!\"\n",
      "Line 1025: Tim did not want to get up. He said, \"No, Sun! I am grumpy! I want to sleep more!\" But the sun did not listen. It kept shining in Tim's face, making it hard to sleep.\n",
      "Line 1026: Tim got up from his bed and went outside. He saw his friend, the cat. The cat was not grumpy. The cat was happy to see Tim. They played together all day. Tim was not grumpy anymore. He was happy, too.\n",
      "Line 1027: <|endoftext|>\n",
      "Line 1028: \n",
      "Line 1029: Once upon a time, there was a little girl. She had the most beautiful skin, not like anyone else's. Everyone was envious of her skin, and she was always so proud of it. One day, something strange happened. Her skin began to change as if it had a mind of its own. She was so scared and confused, until she saw a magical creature. He told her it was time for a new change and her beautiful skin was going to be even more special. With the wave of his wand, her skin glowed a brilliant rainbow color and everyone couldn't believe it! The little girl was so happy to embrace her new skin, and everyone who once was envious of her skin was even more amazed.\n",
      "Line 1030: <|endoftext|>\n",
      "Line 1031: Once upon a time, there was a purple cat. The cat loved to skip all day. It would skip in the sun and in the rain. The purple cat was very happy.\n",
      "Line 1032: One day, the purple cat met a foolish dog. The dog did not know how to skip. The cat wanted to help the dog learn how to skip too. They tried and tried, but the dog could not skip like the cat.\n",
      "Line 1033: The purple cat and the foolish dog became friends. They played together every day. The cat would skip and the dog would run. They were happy and had lots of fun. And they lived happily ever after.\n",
      "Line 1034: <|endoftext|>\n",
      "Line 1035: Once upon a time, there was a barber. He cut hair with big scissors. People liked him because he made them look nice. But his haircuts were very expensive. They cost a lot of money.\n",
      "Line 1036: One day, a little dog came into the barber's shop. The dog had very long hair. It could not see where it was going. The barber wanted to help the dog. He thought, \"I will cut the dog's hair so it can see.\"\n",
      "Line 1037: The barber started to cut the dog's hair. But the dog did not want a haircut. It was scared of the big scissors. So, the dog tried to escape. It ran out of the barber's shop very fast.\n",
      "Line 1038: The barber felt sad. He wanted to help the dog. But he knew the dog was scared. So, he thought of a new way to help the dog. He bought a toy. The toy looked like big scissors, but it was not sharp. The dog saw the toy and was not scared anymore. The barber cut the dog's hair with the toy scissors. The dog was happy, and the barber was happy too.\n",
      "Line 1039: <|endoftext|>\n",
      "Line 1040: \n",
      "Line 1041: Once upon a time there was a little boy named Jacob. Jacob loved to dream of what he wanted in life. One day he was playing in his backyard when he suddenly noticed something strange in the bushes. It was a bright, harmless whip! Jacob was overjoyed!\n",
      "Line 1042: Jacob ran to his mom and said, “Mommy, I found a Whip! Can I keep it?” His mom replied, “No, Jacob, that whip is not for you. It could be dangerous and you could hurt yourself.” Jacob thought about what his mom said and decided she was right. He put the whip back and went off to play.\n",
      "Line 1043: Jacob thought about this experience for a long time. He realized that even harmless things can be dangerous if used in the wrong way. He made a promise to himself to always think before he acts.\n",
      "Line 1044: The moral of this story is to think before you act. If Jacob had taken the whip without thinking, it could have caused some serious harm. We must always be thoughtful of the choices we make!\n",
      "Line 1045: <|endoftext|>\n",
      "Line 1046: \n",
      "Line 1047: Once upon a time there was a little boy. He loved to imagine things. One day, he imagined a deep lip sticking out of the ground. He ran to it and tried to jump on it, but it was too slippery. He tried again and again, but the lip was too deep and he couldn't reach it.\n",
      "Line 1048: The little boy started to cry. He wanted to get on top of the lip and explore.\n",
      "Line 1049: His mum saw him crying and said, \"Don't worry, my dear. You just need to keep trying.\"\n",
      "Line 1050: The little boy tried again and again, but with no luck. He was about to give up and go home when suddenly he slipped and fell into the deep lip.\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/scratch/shayan/Projects/LLMfromScratch/data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 1000:\n",
    "            continue\n",
    "        if i >= 1050:\n",
    "            break\n",
    "        print(f\"Line {i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a304a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22493387"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e18a297a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre-tokenizing the vocabulary:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre-tokenizing the vocabulary: 100%|██████████| 23/23 [00:01<00:00, 11.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# pre-tokenize the data regex-based GPT-2 style \n",
    "from tqdm import tqdm\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "TOKEN_BYTES = b\"<|endoftext|>\"\n",
    "\n",
    "chunk_size = 1000000\n",
    "tokens = []\n",
    "\n",
    "for i in tqdm(range(0, len(data), chunk_size), desc=\"pre-tokenizing the vocabulary\"):\n",
    "    chunk = data[i:i+chunk_size]\n",
    "    tokens.extend(re.findall(PAT, chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a42ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as pyre\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "_COMP = None\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "SEP_PAT = pyre.compile(\"|\".join(map(pyre.escape, sorted(special_tokens, key=len, reverse=True))))\n",
    "\n",
    "def _tokenize_slice(args: Tuple[str, int, int, str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    open file at a path, read bytes, decode, regex-tokenize, return tokens.\n",
    "    \"\"\"\n",
    "    global _COMP\n",
    "    path, start, end, pattern = args\n",
    "    if _COMP is None:\n",
    "        _COMP = re.compile(pattern)\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end-start).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    counts = Counter()\n",
    "    for doc in (d for d in SEP_PAT.split(chunk) if d and d not in special_tokens):\n",
    "        counts.update(_COMP.findall(doc))\n",
    "\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0850925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_tokenize_file(data_path: str, desired_num_chunks: int = None, max_workers: int = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    splits the file on TOKEN_BYTES boundaries, then tokenizes chunks in parallel.\n",
    "    Returns a single flat list of tokens. \n",
    "    \"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = max(1, (os.cpu_count() or 4) - 1)\n",
    "    \n",
    "    if desired_num_chunks is None:\n",
    "        desired_num_chunks = max_workers * 3\n",
    "    \n",
    "    with open(data_path, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(f, desired_num_chunks, TOKEN_BYTES)\n",
    "\n",
    "    pairs = list(zip(boundaries[:-1], boundaries[1:]))\n",
    "    tasks = ((data_path, s, e, PAT) for s, e in pairs)\n",
    "\n",
    "    token_counts = Counter()\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = [ex.submit(_tokenize_slice, t) for t in tasks]\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"tokenizing chunks\"):\n",
    "            token_counts.update(fut.result())\n",
    "\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4543df64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing chunks: 100%|██████████| 24/24 [00:00<00:00, 73.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 5,419,001\n",
      "[('.', 421616), (',', 235432), (' the', 211031), (' and', 196057), (' a', 152161)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "token_counts = parallelize_tokenize_file(data_path, desired_num_chunks=24, max_workers=8)\n",
    "\n",
    "total_tokens = sum(token_counts.values())\n",
    "top20 = token_counts.most_common(20)\n",
    "\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(top20[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92c5614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "\n",
    "class BytePairEncodingTokenizer():\n",
    "    def __init__(self, data_path):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.merges = []\n",
    "        self.merge_ranks = {}\n",
    "        self.b2u, self.u2b = self._bytes_to_unicode()\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = []\n",
    "\n",
    "    def _bytes_to_unicode(self):\n",
    "        # visible ranges (don’t collide with space or control chars)\n",
    "        bs = list(range(ord('!'), ord('~')+1)) + \\\n",
    "            list(range(ord('¡'), ord('¬')+1)) + \\\n",
    "            list(range(ord('®'), ord('ÿ')+1))\n",
    "        cs = bs[:]\n",
    "        n = 0\n",
    "        for b in range(256):\n",
    "            if b not in bs:\n",
    "                bs.append(b)\n",
    "                cs.append(256 + n)  # map leftover bytes to safe code points\n",
    "                n += 1\n",
    "        b2u = {b: chr(c) for b, c in zip(bs, cs)}   # byte -> unicode char\n",
    "        u2b = {v: k for k, v in b2u.items()}        # unicode char -> byte\n",
    "        return b2u, u2b\n",
    "    \n",
    "    def initialize_vocabulary(self, special_tokens):\n",
    "        self.token_to_id = {tok: i for i, tok in enumerate(special_tokens)}\n",
    "        self.id_to_token = special_tokens[:]\n",
    "        \n",
    "        # base byte tokens (each is a single printable Unicode \"byte-char\")\n",
    "        for b in range(256):\n",
    "            tok = self.b2u[b]\n",
    "            self.token_to_id[tok] = len(self.id_to_token)\n",
    "            self.id_to_token.append(tok)\n",
    "\n",
    "        self.merges = []\n",
    "\n",
    "    def _add_merge(\n",
    "            self,\n",
    "            a: str, \n",
    "            b: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add a new merge token a+b; update vocab; return (token, id)\n",
    "        \"\"\"\n",
    "        new_token = a + b\n",
    "        if new_token not in self.token_to_id:\n",
    "            new_id = len(self.id_to_token)\n",
    "            self.token_to_id[new_token] = new_id\n",
    "            self.id_to_token.append(new_token)\n",
    "            self.merges.append((a, b))\n",
    "            self.merge_ranks[(a, b)] = len(self.merges) - 1\n",
    "            return new_token, new_id\n",
    "        \n",
    "        # if merged token already present\n",
    "        return new_token, self.token_to_id[new_token]\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_all_pairs(s):\n",
    "        return list(zip(s, s[1:]))\n",
    "\n",
    "    \n",
    "    def find_most_frequent_pair(self, token_counts: dict) -> list[tuple]:\n",
    "        all_token_counts = Counter()\n",
    "        for token, count in token_counts.items():\n",
    "            if len(token) < 2:\n",
    "                continue\n",
    "\n",
    "            pairs = self._find_all_pairs(token)\n",
    "            local_counts = Counter(pairs)\n",
    "\n",
    "            all_token_counts.update({k: v * count for k, v in local_counts.items()})\n",
    "\n",
    "        return all_token_counts.most_common(1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _apply_merge_to_seq(seq, a, b, ab):\n",
    "        out = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(seq):\n",
    "            if i + 1 < len(seq) and seq[i] == a and seq[i+1] == b:\n",
    "                out.append(ab)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(seq[i])\n",
    "                i += 1\n",
    "            \n",
    "        return tuple(out)\n",
    "        \n",
    "    def train_bpe(\n",
    "            self, input_path: str, vocab_size: int, special_tokens: list[str]\n",
    "    ) -> tuple:\n",
    "        # look at byte pairs\n",
    "        \n",
    "        \n",
    "        # return merges: List[tuple[bytes, bytes]] a list of BPE merges\n",
    "        self.initialize_vocabulary(special_tokens)\n",
    "        print(f\"Vocabulary Length: {len(self.token_to_id)}\")\n",
    "        token_counts = parallelize_tokenize_file(input_path, desired_num_chunks=24, max_workers=8)\n",
    "        corpus = {\n",
    "            tuple(self.b2u[b] for b in ud.normalize(\"NFC\", s).encode(\"utf-8\")): freq\n",
    "            for s, freq in token_counts.items()\n",
    "        }\n",
    "        \n",
    "        target_merges = max(0, vocab_size - len(self.token_to_id))\n",
    "        with tqdm(total=target_merges, dynamic_ncols=True, desc=\"Training BPE...\") as pbar:\n",
    "            while len(self.token_to_id) < vocab_size:\n",
    "                pair = self.find_most_frequent_pair(corpus)[0][0]\n",
    "                if pair is None:\n",
    "                    break # no more mergable pairs; stop early\n",
    "\n",
    "                a, b = pair    \n",
    "                ab, _ = self._add_merge(a, b)\n",
    "                new_corpus = {}\n",
    "                for seq, freq in corpus.items():\n",
    "                    new_seq = self._apply_merge_to_seq(seq, a, b, ab)\n",
    "                    new_corpus[new_seq] = new_corpus.get(new_seq, 0) + freq\n",
    "\n",
    "                corpus = new_corpus\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix_str(f\"last merge: {len(ab)} chars\")\n",
    "        \n",
    "        return self.token_to_id, self.merges\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe12225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_bpe(vocab, merges, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if isinstance(vocab, dict):\n",
    "        vocab_json = vocab\n",
    "    else:\n",
    "        raise TypeError(\"Vocabulary must be a dict of token->id\")\n",
    "    \n",
    "    with (output_dir/\"vocab.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vocab_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    merges_path = output_dir / \"merges.txt\"\n",
    "    with merges_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for a, b in merges:\n",
    "            f.write(f\"{a} {b}\\n\")\n",
    "\n",
    "def train_bpe_tinystories(data_path, vocab_size=1000, special_tokens=[\"<|endoftext|>\"], out_dir=\"tokenizer\"):\n",
    "    bpe = BytePairEncodingTokenizer(data_path)\n",
    "    vocabulary, merges = bpe.train_bpe(data_path, vocab_size=vocab_size, special_tokens=[\"<|endoftext|>\"])\n",
    "    save_bpe(vocabulary, merges, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b022e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length: 257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing chunks: 100%|██████████| 24/24 [00:00<00:00, 74.87it/s]\n",
      "Training BPE...: 100%|██████████| 743/743 [00:40<00:00, 18.35it/s, last merge: 10 chars]\n"
     ]
    }
   ],
   "source": [
    "train_bpe_tinystories(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e4296e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
