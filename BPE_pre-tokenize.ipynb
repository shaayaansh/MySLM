{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e349d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from typing import Tuple, List, Iterable, BinaryIO\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e11b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1001: One day, a big dog named Max saw a small cat named Lily on top of a tree. Lily was angry because she could not get down. Max wanted to help Lily, so he thought of a plan.\n",
      "Line 1002: Max said, \"Lily, I will join you on top of the tree and help you get down.\" Max climbed up the tree and slowly got closer to Lily. Lily was scared at first, but Max was kind and gentle.\n",
      "Line 1003: Max said, \"Hold on to me, Lily. I will take you down.\" Lily held on tight to Max, and they went down the tree together. Lily was happy and thanked Max for helping her. From that day on, Max and Lily became the best of friends.\n",
      "Line 1004: <|endoftext|>\n",
      "Line 1005: Once upon a time, there was a white shark. The white shark lived in the big sea. One day, the white shark saw a little boat. The little boat had a hole in it. The white shark wanted to help.\n",
      "Line 1006: The white shark swam to the boat. The white shark said, \"I can fix your boat.\" The man in the boat was scared. The man said, \"No, go away!\" The white shark was sad but still wanted to help.\n",
      "Line 1007: The white shark tried to fix the hole with its nose. But the boat broke more. The man fell into the water. The white shark was very sorry, but it was too late. The boat was gone and the man was sad.\n",
      "Line 1008: <|endoftext|>\n",
      "Line 1009: Once upon a time, there was a dependable dog named Spot. Spot loved to play with his best friend, a little girl named Lucy. One day, they were playing in the yard when they found a big pear tree.\n",
      "Line 1010: Lucy said, \"Look, Spot! A pear tree! Let's pick a pear!\" They picked the biggest pear they could reach. The pear was very big and very yummy-looking. Spot and Lucy wanted to share the pear with their friends, so they went to find them.\n",
      "Line 1011: They found their friends, a cat named Whiskers and a bird named Tweetie, playing nearby. Lucy said, \"We found a big pear! Let's all share it!\" Whiskers and Tweetie were happy to share the pear. They all took turns taking bites of the pear, and they all agreed that it was the best pear they had ever tasted.\n",
      "Line 1012: As they were eating, a squirrel came up to them and said, \"That pear belongs to me! I picked it and dropped it by accident.\" Lucy, Spot, Whiskers, and Tweetie felt bad for taking the squirrel's pear. They all said sorry to the squirrel.\n",
      "Line 1013: The squirrel said, \"It's okay, we can all share the pear.\" So, they all sat together and finished eating the big, yummy pear. They all became friends and played together every day under the pear tree.\n",
      "Line 1014: <|endoftext|>\n",
      "Line 1015: \n",
      "Line 1016: It was a cold winter day. The snow covered the ground and the tall trees.\n",
      "Line 1017: Jason, who was three years old, wanted to stay outside and play in the snow. He wrapped his warm coat closer to his body and built a snowman with his dad.\n",
      "Line 1018: \"Look daddy!\" he shouted, pointing at the snowman.\n",
      "Line 1019: Dad smiled. \"It looks great.\"\n",
      "Line 1020: Just then, Jason saw a small smoke coming out of a chimney of the nearest house. He said, \"Let's hope someone will give us hot chocolate.\"\n",
      "Line 1021: Dad smiled again. \"That would be nice wouldn't it?\"\n",
      "Line 1022: Jason smiled and nodded. \"Yes, please. I'm so weak from playing in the snow.\"\n",
      "Line 1023: But when they reached the house, nobody answered the door. Jason was disappointed.\n",
      "Line 1024: \"Let's hope someone else will help us,\" said Dad.\n",
      "Line 1025: Just then, an old man came out of a nearby house. He said, \"Why, hello there. Let's warm up with some hot chocolate!\"\n",
      "Line 1026: Jason smiled. He was very happy. Now he was hopeful that he could still get his hot chocolate!\n",
      "Line 1027: <|endoftext|>\n",
      "Line 1028: \n",
      "Line 1029: Once upon a time, there was a poor man called Joe. He had a mustache and was very nice. One day, Joe went to the park to meet a friend.\n",
      "Line 1030: When Joe got to the park, he saw a little girl. She was sitting on the swings, crying.\n",
      "Line 1031: Joe went over to the little girl and asked her why she was crying. The girl told Joe that she was sad because she didn't know her own name.\n",
      "Line 1032: Joe was very kind and gentle and said to the little girl, \"Don't worry. I will help you find your name.\"\n",
      "Line 1033: Joe and the little girl searched all around the park, but they couldn't find her name. Joe was getting tired and the little girl was starting to cry again.\n",
      "Line 1034: Then, suddenly, Joe remembered something. He put his hand on the little girl's head and said, \"I know your name! Your name is Julie!\"\n",
      "Line 1035: The little girl smiled and said, \"Thank you Joe for finding my name!\" Joe smiled back, patted her on the head and said, \"You're welcome\".\n",
      "Line 1036: <|endoftext|>\n",
      "Line 1037: \n",
      "Line 1038: \n",
      "Line 1039: Ben and Lily liked to design things with their blocks. They made houses, cars, towers and animals. They had fun playing with their designs and making noises.\n",
      "Line 1040: One day, they wanted to design a big castle. They used all their blocks and worked hard. They made walls, doors, windows and a flag. They were proud of their castle and wanted to show it to their mom.\n",
      "Line 1041: But when they went to look for their mom, they saw a young boy in their room. He was their cousin, Tom. He had come to visit with his mom. He was bored and saw the blocks. He did not know that Ben and Lily had designed a castle. He thought they were just blocks. He knocked down the castle and threw the blocks around. He laughed and said, \"This is fun!\"\n",
      "Line 1042: Ben and Lily came back and saw what Tom had done. They were very sad and angry. They shouted, \"Tom, you are bad! You broke our castle! We worked hard to design it! Go away!\"\n",
      "Line 1043: Tom did not care. He said, \"I don't care. They are just blocks. You can make another castle. I want to play with them. Give them to me!\"\n",
      "Line 1044: He tried to grab the blocks from Ben and Lily. They did not want to share. They pushed him away. They started to fight. They hit, kicked and bit each other. They cried and screamed.\n",
      "Line 1045: Their mom heard the noise and came to see what was wrong. She saw the mess and the fight. She was very upset and disappointed. She said, \"Stop it, all of you! You are being naughty and rude! You are not playing nicely! You are making me unhappy!\"\n",
      "Line 1046: She took away the blocks and put them in a box. She said, \"You cannot play with the blocks anymore. You have to go to your rooms and think about what you did. You have to say sorry to each other and to me. You have to learn to share and respect each other's designs.\"\n",
      "Line 1047: She sent them to their rooms. They felt uncomfortable and ashamed. They did not want to say sorry. They did not want to share. They did not want to design anything anymore. They had lost their fun and their castle. They had a bad day.\n",
      "Line 1048: <|endoftext|>\n",
      "Line 1049: \n",
      "Line 1050: Once upon a time, there was a strong lion. He had a problem. He wanted to build a big tall house in his jungle, but he was stuck. He didn't know how to solve it.\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/scratch/shayan/Projects/LLMfromScratch/data/TinyStoriesV2-GPT4-train.txt\"\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 1000:\n",
    "            continue\n",
    "        if i >= 1050:\n",
    "            break\n",
    "        print(f\"Line {i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a304a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2226845268"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e18a297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-tokenize the data regex-based GPT-2 style \n",
    "from tqdm import tqdm\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "TOKEN_BYTES = b\"<|endoftext|>\"\n",
    "\n",
    "# chunk_size = 1000000\n",
    "# tokens = []\n",
    "\n",
    "# for i in tqdm(range(0, len(data), chunk_size), desc=\"pre-tokenizing the vocabulary\"):\n",
    "#     chunk = data[i:i+chunk_size]\n",
    "#     tokens.extend(re.findall(PAT, chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a42ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as pyre\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "_COMP = None\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "SEP_PAT = pyre.compile(\"|\".join(map(pyre.escape, sorted(special_tokens, key=len, reverse=True))))\n",
    "\n",
    "def _tokenize_slice(args: Tuple[str, int, int, str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    open file at a path, read bytes, decode, regex-tokenize, return tokens.\n",
    "    \"\"\"\n",
    "    global _COMP\n",
    "    path, start, end, pattern = args\n",
    "    if _COMP is None:\n",
    "        _COMP = re.compile(pattern)\n",
    "\n",
    "    with open(path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end-start).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    counts = Counter()\n",
    "    for doc in (d for d in SEP_PAT.split(chunk) if d and d not in special_tokens):\n",
    "        counts.update(_COMP.findall(doc))\n",
    "\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0850925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_tokenize_file(data_path: str, desired_num_chunks: int = None, max_workers: int = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    splits the file on TOKEN_BYTES boundaries, then tokenizes chunks in parallel.\n",
    "    Returns a single flat list of tokens. \n",
    "    \"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = max(1, (os.cpu_count() or 4) - 1)\n",
    "    \n",
    "    if desired_num_chunks is None:\n",
    "        desired_num_chunks = max_workers * 3\n",
    "    \n",
    "    with open(data_path, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(f, desired_num_chunks, TOKEN_BYTES)\n",
    "\n",
    "    pairs = list(zip(boundaries[:-1], boundaries[1:]))\n",
    "    tasks = ((data_path, s, e, PAT) for s, e in pairs)\n",
    "\n",
    "    token_counts = Counter()\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = [ex.submit(_tokenize_slice, t) for t in tasks]\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"tokenizing chunks\"):\n",
    "            token_counts.update(fut.result())\n",
    "\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4543df64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing chunks: 100%|██████████| 24/24 [00:26<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 536,592,168\n",
      "[('.', 41764510), (',', 23284330), (' the', 20828576), (' and', 19475966), (' a', 15063529)]\n"
     ]
    }
   ],
   "source": [
    "token_counts = parallelize_tokenize_file(data_path, desired_num_chunks=24, max_workers=8)\n",
    "\n",
    "total_tokens = sum(token_counts.values())\n",
    "top20 = token_counts.most_common(20)\n",
    "\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(top20[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "\n",
    "class BytePairEncodingTokenizer():\n",
    "    def __init__(self, data_path):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.merges = []\n",
    "        self.merge_ranks = {}\n",
    "        self.b2u, self.u2b = self._bytes_to_unicode()\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = []\n",
    "\n",
    "    def _bytes_to_unicode(self):\n",
    "        # visible ranges (don’t collide with space or control chars)\n",
    "        bs = list(range(ord('!'), ord('~')+1)) + \\\n",
    "            list(range(ord('¡'), ord('¬')+1)) + \\\n",
    "            list(range(ord('®'), ord('ÿ')+1))\n",
    "        cs = bs[:]\n",
    "        n = 0\n",
    "        for b in range(256):\n",
    "            if b not in bs:\n",
    "                bs.append(b)\n",
    "                cs.append(256 + n)  # map leftover bytes to safe code points\n",
    "                n += 1\n",
    "        b2u = {b: chr(c) for b, c in zip(bs, cs)}   # byte -> unicode char\n",
    "        u2b = {v: k for k, v in b2u.items()}        # unicode char -> byte\n",
    "        return b2u, u2b\n",
    "    \n",
    "    def initialize_vocabulary(self, special_tokens):\n",
    "        self.token_to_id = {tok: i for i, tok in enumerate(special_tokens)}\n",
    "        self.id_to_token = special_tokens[:]\n",
    "        \n",
    "        # base byte tokens (each is a single printable Unicode \"byte-char\")\n",
    "        for b in range(256):\n",
    "            tok = self.b2u[b]\n",
    "            self.token_to_id[tok] = len(self.id_to_token)\n",
    "            self.id_to_token.append(tok)\n",
    "\n",
    "        self.merges = []\n",
    "\n",
    "    def _add_merge(\n",
    "            self,\n",
    "            a: str, \n",
    "            b: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add a new merge token a+b; update vocab; return (token, id)\n",
    "        \"\"\"\n",
    "        new_token = a + b\n",
    "        if new_token not in self.token_to_id:\n",
    "            new_id = len(self.id_to_token)\n",
    "            self.token_to_id[new_token] = new_id\n",
    "            self.id_to_token.append(new_token)\n",
    "            self.merges.append((a, b))\n",
    "            self.merge_ranks[(a, b)] = len(self.merges) - 1\n",
    "            return new_token, new_id\n",
    "        \n",
    "        # if merged token already present\n",
    "        return new_token, self.token_to_id[new_token]\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_all_pairs(s):\n",
    "        return list(zip(s, s[1:]))\n",
    "\n",
    "    \n",
    "    def find_most_frequent_pair(self, token_counts: dict) -> list[tuple]:\n",
    "        all_token_counts = Counter()\n",
    "        for token, count in token_counts.items():\n",
    "            if len(token) < 2:\n",
    "                continue\n",
    "\n",
    "            pairs = self._find_all_pairs(token)\n",
    "            local_counts = Counter(pairs)\n",
    "\n",
    "            all_token_counts.update({k: v * count for k, v in local_counts.items()})\n",
    "\n",
    "        return all_token_counts.most_common(1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _apply_merge_to_seq(seq, a, b, ab):\n",
    "        out = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(seq):\n",
    "            if i + 1 < len(seq) and seq[i] == a and seq[i+1] == b:\n",
    "                out.append(ab)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(seq[i])\n",
    "                i += 1\n",
    "            \n",
    "        return tuple(out)\n",
    "        \n",
    "    def train_bpe(\n",
    "            self, input_path: str, vocab_size: int, special_tokens: list[str]\n",
    "    ) -> tuple:\n",
    "        # initialize the vocabulary\n",
    "        self.initialize_vocabulary(special_tokens)\n",
    "        print(f\"Initial Vocabulary Length: {len(self.token_to_id)}\")\n",
    "        token_counts = parallelize_tokenize_file(input_path, desired_num_chunks=24, max_workers=8)\n",
    "        \n",
    "        corpus = {\n",
    "            tuple(self.b2u[b] for b in ud.normalize(\"NFC\", s).encode(\"utf-8\")): freq\n",
    "            for s, freq in token_counts.items()\n",
    "        }\n",
    "        \n",
    "        target_merges = max(0, vocab_size - len(self.token_to_id))\n",
    "        with tqdm(total=target_merges, dynamic_ncols=True, desc=\"Training BPE...\") as pbar:\n",
    "            while len(self.token_to_id) < vocab_size:\n",
    "                pair = self.find_most_frequent_pair(corpus)[0][0]\n",
    "                if pair is None:\n",
    "                    break # no more mergable pairs; stop early\n",
    "\n",
    "                a, b = pair    \n",
    "                ab, _ = self._add_merge(a, b)\n",
    "                new_corpus = {}\n",
    "                for seq, freq in corpus.items():\n",
    "                    new_seq = self._apply_merge_to_seq(seq, a, b, ab)\n",
    "                    new_corpus[new_seq] = new_corpus.get(new_seq, 0) + freq\n",
    "\n",
    "                corpus = new_corpus\n",
    "\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix_str(f\"last merge: {len(ab)} chars\")\n",
    "        \n",
    "        return self.token_to_id, self.merges\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_bpe(vocab, merges, output_dir, data_name):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if isinstance(vocab, dict):\n",
    "        vocab_json = vocab\n",
    "    else:\n",
    "        raise TypeError(\"Vocabulary must be a dict of token->id\")\n",
    "    \n",
    "    with (output_dir/f\"{data_name}_vocab.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(vocab_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    merges_path = output_dir / f\"{data_name}_merges.txt\"\n",
    "    with merges_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for a, b in merges:\n",
    "            f.write(f\"{a} {b}\\n\")\n",
    "\n",
    "def train_bpe_tinystories(data_path, vocab_size=10000, special_tokens=[\"<|endoftext|>\"], out_dir=\"tokenizer\"):\n",
    "    bpe = BytePairEncodingTokenizer(data_path)\n",
    "    vocabulary, merges = bpe.train_bpe(data_path, vocab_size=vocab_size, special_tokens=[\"<|endoftext|>\"])\n",
    "    save_bpe(vocabulary, merges, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b022e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length: 257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing chunks: 100%|██████████| 24/24 [00:26<00:00,  1.08s/it]\n",
      "Training BPE...: 100%|██████████| 9743/9743 [34:15<00:00,  4.74it/s, last merge: 10 chars]\n"
     ]
    }
   ],
   "source": [
    "train_bpe_tinystories(data_path, vocab_size=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
